{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLG Assingment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"poland-bankruptcy-data-2009.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_56</th>\n",
       "      <th>feat_57</th>\n",
       "      <th>feat_58</th>\n",
       "      <th>feat_59</th>\n",
       "      <th>feat_60</th>\n",
       "      <th>feat_61</th>\n",
       "      <th>feat_62</th>\n",
       "      <th>feat_63</th>\n",
       "      <th>feat_64</th>\n",
       "      <th>bankrupt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.174190</td>\n",
       "      <td>0.41299</td>\n",
       "      <td>0.14371</td>\n",
       "      <td>1.3480</td>\n",
       "      <td>-28.9820</td>\n",
       "      <td>0.60383</td>\n",
       "      <td>0.219460</td>\n",
       "      <td>1.12250</td>\n",
       "      <td>1.1961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163960</td>\n",
       "      <td>0.375740</td>\n",
       "      <td>0.83604</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.7145</td>\n",
       "      <td>6.2813</td>\n",
       "      <td>84.291</td>\n",
       "      <td>4.3303</td>\n",
       "      <td>4.0341</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.146240</td>\n",
       "      <td>0.46038</td>\n",
       "      <td>0.28230</td>\n",
       "      <td>1.6294</td>\n",
       "      <td>2.5952</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.171850</td>\n",
       "      <td>1.17210</td>\n",
       "      <td>1.6018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.90108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.9882</td>\n",
       "      <td>4.1103</td>\n",
       "      <td>102.190</td>\n",
       "      <td>3.5716</td>\n",
       "      <td>5.9500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.22612</td>\n",
       "      <td>0.48839</td>\n",
       "      <td>3.1599</td>\n",
       "      <td>84.8740</td>\n",
       "      <td>0.19114</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>2.98810</td>\n",
       "      <td>1.0077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.99236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.7742</td>\n",
       "      <td>3.7922</td>\n",
       "      <td>64.846</td>\n",
       "      <td>5.6287</td>\n",
       "      <td>4.4581</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.188290</td>\n",
       "      <td>0.41504</td>\n",
       "      <td>0.34231</td>\n",
       "      <td>1.9279</td>\n",
       "      <td>-58.2740</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.233580</td>\n",
       "      <td>1.40940</td>\n",
       "      <td>1.3393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>0.321880</td>\n",
       "      <td>0.82635</td>\n",
       "      <td>0.073039</td>\n",
       "      <td>2.5912</td>\n",
       "      <td>7.0756</td>\n",
       "      <td>100.540</td>\n",
       "      <td>3.6303</td>\n",
       "      <td>4.6375</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.182060</td>\n",
       "      <td>0.55615</td>\n",
       "      <td>0.32191</td>\n",
       "      <td>1.6045</td>\n",
       "      <td>16.3140</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.182060</td>\n",
       "      <td>0.79808</td>\n",
       "      <td>1.8126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555770</td>\n",
       "      <td>0.410190</td>\n",
       "      <td>0.46957</td>\n",
       "      <td>0.029421</td>\n",
       "      <td>8.4553</td>\n",
       "      <td>3.3488</td>\n",
       "      <td>107.240</td>\n",
       "      <td>3.4036</td>\n",
       "      <td>12.4540</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   company_id    feat_1   feat_2   feat_3  feat_4   feat_5   feat_6    feat_7  \\\n",
       "0           1  0.174190  0.41299  0.14371  1.3480 -28.9820  0.60383  0.219460   \n",
       "1           2  0.146240  0.46038  0.28230  1.6294   2.5952  0.00000  0.171850   \n",
       "2           3  0.000595  0.22612  0.48839  3.1599  84.8740  0.19114  0.004572   \n",
       "3           5  0.188290  0.41504  0.34231  1.9279 -58.2740  0.00000  0.233580   \n",
       "4           6  0.182060  0.55615  0.32191  1.6045  16.3140  0.00000  0.182060   \n",
       "\n",
       "    feat_8  feat_9  ...   feat_56   feat_57  feat_58   feat_59  feat_60  \\\n",
       "0  1.12250  1.1961  ...  0.163960  0.375740  0.83604  0.000007   9.7145   \n",
       "1  1.17210  1.6018  ...  0.027516  0.271000  0.90108  0.000000   5.9882   \n",
       "2  2.98810  1.0077  ...  0.007639  0.000881  0.99236  0.000000   6.7742   \n",
       "3  1.40940  1.3393  ...  0.176480  0.321880  0.82635  0.073039   2.5912   \n",
       "4  0.79808  1.8126  ...  0.555770  0.410190  0.46957  0.029421   8.4553   \n",
       "\n",
       "   feat_61  feat_62  feat_63  feat_64  bankrupt  \n",
       "0   6.2813   84.291   4.3303   4.0341     False  \n",
       "1   4.1103  102.190   3.5716   5.9500     False  \n",
       "2   3.7922   64.846   5.6287   4.4581     False  \n",
       "3   7.0756  100.540   3.6303   4.6375     False  \n",
       "4   3.3488  107.240   3.4036  12.4540     False  \n",
       "\n",
       "[5 rows x 66 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame().from_dict(data['data'])\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_id</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_55</th>\n",
       "      <th>feat_56</th>\n",
       "      <th>feat_57</th>\n",
       "      <th>feat_58</th>\n",
       "      <th>feat_59</th>\n",
       "      <th>feat_60</th>\n",
       "      <th>feat_61</th>\n",
       "      <th>feat_62</th>\n",
       "      <th>feat_63</th>\n",
       "      <th>feat_64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9960.000000</td>\n",
       "      <td>9.952000e+03</td>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9964.000000</td>\n",
       "      <td>9974.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.977000e+03</td>\n",
       "      <td>9935.000000</td>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9948.000000</td>\n",
       "      <td>9977.000000</td>\n",
       "      <td>9.415000e+03</td>\n",
       "      <td>9961.00000</td>\n",
       "      <td>9.935000e+03</td>\n",
       "      <td>9960.000000</td>\n",
       "      <td>9765.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5252.399419</td>\n",
       "      <td>0.052688</td>\n",
       "      <td>0.623634</td>\n",
       "      <td>0.088614</td>\n",
       "      <td>10.067323</td>\n",
       "      <td>-1.412557e+03</td>\n",
       "      <td>-0.124340</td>\n",
       "      <td>0.065407</td>\n",
       "      <td>16.757657</td>\n",
       "      <td>1.820532</td>\n",
       "      <td>...</td>\n",
       "      <td>6.771696e+03</td>\n",
       "      <td>-0.561651</td>\n",
       "      <td>-0.028476</td>\n",
       "      <td>3.976806</td>\n",
       "      <td>1.486227</td>\n",
       "      <td>5.938426e+02</td>\n",
       "      <td>13.62412</td>\n",
       "      <td>1.364510e+02</td>\n",
       "      <td>9.034435</td>\n",
       "      <td>36.291450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3030.164482</td>\n",
       "      <td>0.662816</td>\n",
       "      <td>6.590734</td>\n",
       "      <td>6.584543</td>\n",
       "      <td>537.128699</td>\n",
       "      <td>1.216712e+05</td>\n",
       "      <td>7.145680</td>\n",
       "      <td>0.666113</td>\n",
       "      <td>673.845491</td>\n",
       "      <td>7.771473</td>\n",
       "      <td>...</td>\n",
       "      <td>6.121491e+04</td>\n",
       "      <td>57.438557</td>\n",
       "      <td>19.134737</td>\n",
       "      <td>195.155108</td>\n",
       "      <td>79.262784</td>\n",
       "      <td>3.812344e+04</td>\n",
       "      <td>73.22837</td>\n",
       "      <td>2.666901e+04</td>\n",
       "      <td>31.773572</td>\n",
       "      <td>438.506192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-479.730000</td>\n",
       "      <td>0.002080</td>\n",
       "      <td>-1.190300e+07</td>\n",
       "      <td>-508.120000</td>\n",
       "      <td>-17.692000</td>\n",
       "      <td>-2.081800</td>\n",
       "      <td>-1.215700</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.513800e+05</td>\n",
       "      <td>-5691.700000</td>\n",
       "      <td>-1667.300000</td>\n",
       "      <td>-198.690000</td>\n",
       "      <td>-172.070000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-6.59030</td>\n",
       "      <td>-2.336500e+06</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>-0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2636.000000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.255280</td>\n",
       "      <td>0.016341</td>\n",
       "      <td>1.036150</td>\n",
       "      <td>-5.266650e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002265</td>\n",
       "      <td>0.430778</td>\n",
       "      <td>1.011100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.304300e+01</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.875850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.532000e+00</td>\n",
       "      <td>4.49120</td>\n",
       "      <td>4.101200e+01</td>\n",
       "      <td>3.049300</td>\n",
       "      <td>2.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5259.000000</td>\n",
       "      <td>0.042731</td>\n",
       "      <td>0.465920</td>\n",
       "      <td>0.197570</td>\n",
       "      <td>1.600650</td>\n",
       "      <td>1.495950e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050661</td>\n",
       "      <td>1.104150</td>\n",
       "      <td>1.197300</td>\n",
       "      <td>...</td>\n",
       "      <td>8.874800e+02</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.106550</td>\n",
       "      <td>0.953130</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>9.933700e+00</td>\n",
       "      <td>6.67050</td>\n",
       "      <td>7.093200e+01</td>\n",
       "      <td>5.116700</td>\n",
       "      <td>4.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7875.000000</td>\n",
       "      <td>0.123140</td>\n",
       "      <td>0.690060</td>\n",
       "      <td>0.415680</td>\n",
       "      <td>2.950525</td>\n",
       "      <td>5.601725e+01</td>\n",
       "      <td>0.071759</td>\n",
       "      <td>0.141700</td>\n",
       "      <td>2.827425</td>\n",
       "      <td>2.044550</td>\n",
       "      <td>...</td>\n",
       "      <td>4.363300e+03</td>\n",
       "      <td>0.129640</td>\n",
       "      <td>0.271860</td>\n",
       "      <td>0.995923</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>2.077800e+01</td>\n",
       "      <td>10.58400</td>\n",
       "      <td>1.187050e+02</td>\n",
       "      <td>8.846400</td>\n",
       "      <td>9.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10503.000000</td>\n",
       "      <td>52.652000</td>\n",
       "      <td>480.730000</td>\n",
       "      <td>17.708000</td>\n",
       "      <td>53433.000000</td>\n",
       "      <td>6.854400e+05</td>\n",
       "      <td>45.533000</td>\n",
       "      <td>52.652000</td>\n",
       "      <td>53432.000000</td>\n",
       "      <td>740.440000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.380500e+06</td>\n",
       "      <td>293.150000</td>\n",
       "      <td>552.640000</td>\n",
       "      <td>18118.000000</td>\n",
       "      <td>7617.300000</td>\n",
       "      <td>3.660200e+06</td>\n",
       "      <td>4246.70000</td>\n",
       "      <td>1.073500e+06</td>\n",
       "      <td>1974.500000</td>\n",
       "      <td>21499.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         company_id       feat_1       feat_2       feat_3        feat_4  \\\n",
       "count   9977.000000  9977.000000  9977.000000  9977.000000   9960.000000   \n",
       "mean    5252.399419     0.052688     0.623634     0.088614     10.067323   \n",
       "std     3030.164482     0.662816     6.590734     6.584543    537.128699   \n",
       "min        1.000000   -17.692000     0.000000  -479.730000      0.002080   \n",
       "25%     2636.000000     0.000700     0.255280     0.016341      1.036150   \n",
       "50%     5259.000000     0.042731     0.465920     0.197570      1.600650   \n",
       "75%     7875.000000     0.123140     0.690060     0.415680      2.950525   \n",
       "max    10503.000000    52.652000   480.730000    17.708000  53433.000000   \n",
       "\n",
       "             feat_5       feat_6       feat_7        feat_8       feat_9  ...  \\\n",
       "count  9.952000e+03  9977.000000  9977.000000   9964.000000  9974.000000  ...   \n",
       "mean  -1.412557e+03    -0.124340     0.065407     16.757657     1.820532  ...   \n",
       "std    1.216712e+05     7.145680     0.666113    673.845491     7.771473  ...   \n",
       "min   -1.190300e+07  -508.120000   -17.692000     -2.081800    -1.215700  ...   \n",
       "25%   -5.266650e+01     0.000000     0.002265      0.430778     1.011100  ...   \n",
       "50%    1.495950e+00     0.000000     0.050661      1.104150     1.197300  ...   \n",
       "75%    5.601725e+01     0.071759     0.141700      2.827425     2.044550  ...   \n",
       "max    6.854400e+05    45.533000    52.652000  53432.000000   740.440000  ...   \n",
       "\n",
       "            feat_55      feat_56      feat_57       feat_58      feat_59  \\\n",
       "count  9.977000e+03  9935.000000  9977.000000   9948.000000  9977.000000   \n",
       "mean   6.771696e+03    -0.561651    -0.028476      3.976806     1.486227   \n",
       "std    6.121491e+04    57.438557    19.134737    195.155108    79.262784   \n",
       "min   -7.513800e+05 -5691.700000 -1667.300000   -198.690000  -172.070000   \n",
       "25%    1.304300e+01     0.005136     0.007051      0.875850     0.000000   \n",
       "50%    8.874800e+02     0.051546     0.106550      0.953130     0.003356   \n",
       "75%    4.363300e+03     0.129640     0.271860      0.995923     0.249600   \n",
       "max    3.380500e+06   293.150000   552.640000  18118.000000  7617.300000   \n",
       "\n",
       "            feat_60     feat_61       feat_62      feat_63       feat_64  \n",
       "count  9.415000e+03  9961.00000  9.935000e+03  9960.000000   9765.000000  \n",
       "mean   5.938426e+02    13.62412  1.364510e+02     9.034435     36.291450  \n",
       "std    3.812344e+04    73.22837  2.666901e+04    31.773572    438.506192  \n",
       "min    0.000000e+00    -6.59030 -2.336500e+06    -0.000156     -0.000102  \n",
       "25%    5.532000e+00     4.49120  4.101200e+01     3.049300      2.003100  \n",
       "50%    9.933700e+00     6.67050  7.093200e+01     5.116700      4.031400  \n",
       "75%    2.077800e+01    10.58400  1.187050e+02     8.846400      9.574000  \n",
       "max    3.660200e+06  4246.70000  1.073500e+06  1974.500000  21499.000000  \n",
       "\n",
       "[8 rows x 65 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company_id      0\n",
       "feat_1          0\n",
       "feat_2          0\n",
       "feat_3          0\n",
       "feat_4         17\n",
       "             ... \n",
       "feat_61        16\n",
       "feat_62        42\n",
       "feat_63        17\n",
       "feat_64       212\n",
       "bankrupt        0\n",
       "Length: 66, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping Company_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>feat_10</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_56</th>\n",
       "      <th>feat_57</th>\n",
       "      <th>feat_58</th>\n",
       "      <th>feat_59</th>\n",
       "      <th>feat_60</th>\n",
       "      <th>feat_61</th>\n",
       "      <th>feat_62</th>\n",
       "      <th>feat_63</th>\n",
       "      <th>feat_64</th>\n",
       "      <th>bankrupt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.174190</td>\n",
       "      <td>0.41299</td>\n",
       "      <td>0.143710</td>\n",
       "      <td>1.34800</td>\n",
       "      <td>-28.9820</td>\n",
       "      <td>0.603830</td>\n",
       "      <td>0.219460</td>\n",
       "      <td>1.122500</td>\n",
       "      <td>1.19610</td>\n",
       "      <td>0.463590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163960</td>\n",
       "      <td>0.375740</td>\n",
       "      <td>0.83604</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>9.7145</td>\n",
       "      <td>6.2813</td>\n",
       "      <td>84.291</td>\n",
       "      <td>4.3303</td>\n",
       "      <td>4.0341</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146240</td>\n",
       "      <td>0.46038</td>\n",
       "      <td>0.282300</td>\n",
       "      <td>1.62940</td>\n",
       "      <td>2.5952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171850</td>\n",
       "      <td>1.172100</td>\n",
       "      <td>1.60180</td>\n",
       "      <td>0.539620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027516</td>\n",
       "      <td>0.271000</td>\n",
       "      <td>0.90108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.9882</td>\n",
       "      <td>4.1103</td>\n",
       "      <td>102.190</td>\n",
       "      <td>3.5716</td>\n",
       "      <td>5.9500</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.22612</td>\n",
       "      <td>0.488390</td>\n",
       "      <td>3.15990</td>\n",
       "      <td>84.8740</td>\n",
       "      <td>0.191140</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>2.988100</td>\n",
       "      <td>1.00770</td>\n",
       "      <td>0.675660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007639</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.99236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.7742</td>\n",
       "      <td>3.7922</td>\n",
       "      <td>64.846</td>\n",
       "      <td>5.6287</td>\n",
       "      <td>4.4581</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.188290</td>\n",
       "      <td>0.41504</td>\n",
       "      <td>0.342310</td>\n",
       "      <td>1.92790</td>\n",
       "      <td>-58.2740</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.233580</td>\n",
       "      <td>1.409400</td>\n",
       "      <td>1.33930</td>\n",
       "      <td>0.584960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176480</td>\n",
       "      <td>0.321880</td>\n",
       "      <td>0.82635</td>\n",
       "      <td>0.073039</td>\n",
       "      <td>2.5912</td>\n",
       "      <td>7.0756</td>\n",
       "      <td>100.540</td>\n",
       "      <td>3.6303</td>\n",
       "      <td>4.6375</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.182060</td>\n",
       "      <td>0.55615</td>\n",
       "      <td>0.321910</td>\n",
       "      <td>1.60450</td>\n",
       "      <td>16.3140</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.182060</td>\n",
       "      <td>0.798080</td>\n",
       "      <td>1.81260</td>\n",
       "      <td>0.443850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555770</td>\n",
       "      <td>0.410190</td>\n",
       "      <td>0.46957</td>\n",
       "      <td>0.029421</td>\n",
       "      <td>8.4553</td>\n",
       "      <td>3.3488</td>\n",
       "      <td>107.240</td>\n",
       "      <td>3.4036</td>\n",
       "      <td>12.4540</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.58067</td>\n",
       "      <td>-0.223860</td>\n",
       "      <td>0.51658</td>\n",
       "      <td>-31.8660</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.618550</td>\n",
       "      <td>1.01200</td>\n",
       "      <td>0.359170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011834</td>\n",
       "      <td>0.007966</td>\n",
       "      <td>0.98817</td>\n",
       "      <td>0.327360</td>\n",
       "      <td>38.1420</td>\n",
       "      <td>39.1500</td>\n",
       "      <td>42.202</td>\n",
       "      <td>8.6489</td>\n",
       "      <td>5.2646</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>-0.051968</td>\n",
       "      <td>0.55254</td>\n",
       "      <td>0.147150</td>\n",
       "      <td>2.16980</td>\n",
       "      <td>12.7480</td>\n",
       "      <td>-0.051968</td>\n",
       "      <td>-0.034361</td>\n",
       "      <td>0.669830</td>\n",
       "      <td>0.94694</td>\n",
       "      <td>0.370110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056037</td>\n",
       "      <td>-0.140410</td>\n",
       "      <td>1.05600</td>\n",
       "      <td>1.153000</td>\n",
       "      <td>14.4930</td>\n",
       "      <td>9.2851</td>\n",
       "      <td>32.761</td>\n",
       "      <td>11.1410</td>\n",
       "      <td>1.9276</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>-0.135900</td>\n",
       "      <td>0.83954</td>\n",
       "      <td>-0.342010</td>\n",
       "      <td>0.46526</td>\n",
       "      <td>-145.3100</td>\n",
       "      <td>-0.219120</td>\n",
       "      <td>-0.131860</td>\n",
       "      <td>0.191130</td>\n",
       "      <td>1.09990</td>\n",
       "      <td>0.160460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144120</td>\n",
       "      <td>-0.846930</td>\n",
       "      <td>0.85427</td>\n",
       "      <td>0.814200</td>\n",
       "      <td>6.2737</td>\n",
       "      <td>9.6966</td>\n",
       "      <td>212.230</td>\n",
       "      <td>1.7198</td>\n",
       "      <td>1.5659</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>0.009423</td>\n",
       "      <td>0.50028</td>\n",
       "      <td>0.261630</td>\n",
       "      <td>1.52300</td>\n",
       "      <td>-10.1580</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.989900</td>\n",
       "      <td>1.01230</td>\n",
       "      <td>0.495230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012186</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>0.98781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.9675</td>\n",
       "      <td>4.3536</td>\n",
       "      <td>98.240</td>\n",
       "      <td>3.7154</td>\n",
       "      <td>7.8068</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>-0.001775</td>\n",
       "      <td>0.94780</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>1.00450</td>\n",
       "      <td>-50.2210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.055122</td>\n",
       "      <td>2.12680</td>\n",
       "      <td>0.052245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023492</td>\n",
       "      <td>-0.033971</td>\n",
       "      <td>0.97208</td>\n",
       "      <td>2.398500</td>\n",
       "      <td>7.5512</td>\n",
       "      <td>3.9960</td>\n",
       "      <td>141.160</td>\n",
       "      <td>2.5857</td>\n",
       "      <td>12.2250</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9977 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        feat_1   feat_2    feat_3   feat_4    feat_5    feat_6    feat_7  \\\n",
       "0     0.174190  0.41299  0.143710  1.34800  -28.9820  0.603830  0.219460   \n",
       "1     0.146240  0.46038  0.282300  1.62940    2.5952  0.000000  0.171850   \n",
       "2     0.000595  0.22612  0.488390  3.15990   84.8740  0.191140  0.004572   \n",
       "3     0.188290  0.41504  0.342310  1.92790  -58.2740  0.000000  0.233580   \n",
       "4     0.182060  0.55615  0.321910  1.60450   16.3140  0.000000  0.182060   \n",
       "...        ...      ...       ...      ...       ...       ...       ...   \n",
       "9972  0.002861  0.58067 -0.223860  0.51658  -31.8660  0.002861  0.002861   \n",
       "9973 -0.051968  0.55254  0.147150  2.16980   12.7480 -0.051968 -0.034361   \n",
       "9974 -0.135900  0.83954 -0.342010  0.46526 -145.3100 -0.219120 -0.131860   \n",
       "9975  0.009423  0.50028  0.261630  1.52300  -10.1580  0.009423  0.007700   \n",
       "9976 -0.001775  0.94780  0.003729  1.00450  -50.2210  0.000000  0.002565   \n",
       "\n",
       "        feat_8   feat_9   feat_10  ...   feat_56   feat_57  feat_58   feat_59  \\\n",
       "0     1.122500  1.19610  0.463590  ...  0.163960  0.375740  0.83604  0.000007   \n",
       "1     1.172100  1.60180  0.539620  ...  0.027516  0.271000  0.90108  0.000000   \n",
       "2     2.988100  1.00770  0.675660  ...  0.007639  0.000881  0.99236  0.000000   \n",
       "3     1.409400  1.33930  0.584960  ...  0.176480  0.321880  0.82635  0.073039   \n",
       "4     0.798080  1.81260  0.443850  ...  0.555770  0.410190  0.46957  0.029421   \n",
       "...        ...      ...       ...  ...       ...       ...      ...       ...   \n",
       "9972  0.618550  1.01200  0.359170  ...  0.011834  0.007966  0.98817  0.327360   \n",
       "9973  0.669830  0.94694  0.370110  ... -0.056037 -0.140410  1.05600  1.153000   \n",
       "9974  0.191130  1.09990  0.160460  ...  0.144120 -0.846930  0.85427  0.814200   \n",
       "9975  0.989900  1.01230  0.495230  ...  0.012186  0.019027  0.98781  0.000000   \n",
       "9976  0.055122  2.12680  0.052245  ...  0.023492 -0.033971  0.97208  2.398500   \n",
       "\n",
       "      feat_60  feat_61  feat_62  feat_63  feat_64  bankrupt  \n",
       "0      9.7145   6.2813   84.291   4.3303   4.0341     False  \n",
       "1      5.9882   4.1103  102.190   3.5716   5.9500     False  \n",
       "2      6.7742   3.7922   64.846   5.6287   4.4581     False  \n",
       "3      2.5912   7.0756  100.540   3.6303   4.6375     False  \n",
       "4      8.4553   3.3488  107.240   3.4036  12.4540     False  \n",
       "...       ...      ...      ...      ...      ...       ...  \n",
       "9972  38.1420  39.1500   42.202   8.6489   5.2646      True  \n",
       "9973  14.4930   9.2851   32.761  11.1410   1.9276      True  \n",
       "9974   6.2737   9.6966  212.230   1.7198   1.5659      True  \n",
       "9975   5.9675   4.3536   98.240   3.7154   7.8068      True  \n",
       "9976   7.5512   3.9960  141.160   2.5857  12.2250      True  \n",
       "\n",
       "[9977 rows x 65 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.drop('company_id', axis=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9977 entries, 0 to 9976\n",
      "Data columns (total 66 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   company_id  9977 non-null   int64  \n",
      " 1   feat_1      9977 non-null   float64\n",
      " 2   feat_2      9977 non-null   float64\n",
      " 3   feat_3      9977 non-null   float64\n",
      " 4   feat_4      9960 non-null   float64\n",
      " 5   feat_5      9952 non-null   float64\n",
      " 6   feat_6      9977 non-null   float64\n",
      " 7   feat_7      9977 non-null   float64\n",
      " 8   feat_8      9964 non-null   float64\n",
      " 9   feat_9      9974 non-null   float64\n",
      " 10  feat_10     9977 non-null   float64\n",
      " 11  feat_11     9977 non-null   float64\n",
      " 12  feat_12     9960 non-null   float64\n",
      " 13  feat_13     9935 non-null   float64\n",
      " 14  feat_14     9977 non-null   float64\n",
      " 15  feat_15     9970 non-null   float64\n",
      " 16  feat_16     9964 non-null   float64\n",
      " 17  feat_17     9964 non-null   float64\n",
      " 18  feat_18     9977 non-null   float64\n",
      " 19  feat_19     9935 non-null   float64\n",
      " 20  feat_20     9935 non-null   float64\n",
      " 21  feat_21     9205 non-null   float64\n",
      " 22  feat_22     9977 non-null   float64\n",
      " 23  feat_23     9935 non-null   float64\n",
      " 24  feat_24     9764 non-null   float64\n",
      " 25  feat_25     9977 non-null   float64\n",
      " 26  feat_26     9964 non-null   float64\n",
      " 27  feat_27     9312 non-null   float64\n",
      " 28  feat_28     9765 non-null   float64\n",
      " 29  feat_29     9977 non-null   float64\n",
      " 30  feat_30     9935 non-null   float64\n",
      " 31  feat_31     9935 non-null   float64\n",
      " 32  feat_32     9881 non-null   float64\n",
      " 33  feat_33     9960 non-null   float64\n",
      " 34  feat_34     9964 non-null   float64\n",
      " 35  feat_35     9977 non-null   float64\n",
      " 36  feat_36     9977 non-null   float64\n",
      " 37  feat_37     5499 non-null   float64\n",
      " 38  feat_38     9977 non-null   float64\n",
      " 39  feat_39     9935 non-null   float64\n",
      " 40  feat_40     9960 non-null   float64\n",
      " 41  feat_41     9787 non-null   float64\n",
      " 42  feat_42     9935 non-null   float64\n",
      " 43  feat_43     9935 non-null   float64\n",
      " 44  feat_44     9935 non-null   float64\n",
      " 45  feat_45     9416 non-null   float64\n",
      " 46  feat_46     9960 non-null   float64\n",
      " 47  feat_47     9896 non-null   float64\n",
      " 48  feat_48     9977 non-null   float64\n",
      " 49  feat_49     9935 non-null   float64\n",
      " 50  feat_50     9964 non-null   float64\n",
      " 51  feat_51     9977 non-null   float64\n",
      " 52  feat_52     9896 non-null   float64\n",
      " 53  feat_53     9765 non-null   float64\n",
      " 54  feat_54     9765 non-null   float64\n",
      " 55  feat_55     9977 non-null   float64\n",
      " 56  feat_56     9935 non-null   float64\n",
      " 57  feat_57     9977 non-null   float64\n",
      " 58  feat_58     9948 non-null   float64\n",
      " 59  feat_59     9977 non-null   float64\n",
      " 60  feat_60     9415 non-null   float64\n",
      " 61  feat_61     9961 non-null   float64\n",
      " 62  feat_62     9935 non-null   float64\n",
      " 63  feat_63     9960 non-null   float64\n",
      " 64  feat_64     9765 non-null   float64\n",
      " 65  bankrupt    9977 non-null   bool   \n",
      "dtypes: bool(1), float64(64), int64(1)\n",
      "memory usage: 5.0 MB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting  the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7981, 65)\n",
      "y_train shape: (7981,)\n",
      "X_test shape: (1996, 65)\n",
      "y_test shape: (1996,)\n"
     ]
    }
   ],
   "source": [
    "X = dataframe.iloc[:, :-1]\n",
    "Y = dataframe.iloc[:, -1]\n",
    "# split the dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X,Y,\n",
    "    test_size=0.2, random_state=0)\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train.replace('?',np.NaN,inplace=True )\n",
    "imp=SimpleImputer(missing_values=np.NaN,strategy='mean')\n",
    "idf=pd.DataFrame(imp.fit_transform(X_train))\n",
    "idf.columns=X_train.columns\n",
    "idf.index=X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_id      0\n",
      "feat_1          0\n",
      "feat_2          0\n",
      "feat_3          0\n",
      "feat_4         14\n",
      "             ... \n",
      "feat_60       459\n",
      "feat_61        12\n",
      "feat_62        33\n",
      "feat_63        14\n",
      "feat_64       171\n",
      "Length: 65, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing imbalance of data first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe.bankrupt.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resampling the data so there is more variance to make disicions by using the Random Over-Sampling method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_id       0\n",
      "feat_1           0\n",
      "feat_2           0\n",
      "feat_3           0\n",
      "feat_4          35\n",
      "              ... \n",
      "feat_60       1091\n",
      "feat_61         33\n",
      "feat_62         72\n",
      "feat_63         35\n",
      "feat_64        386\n",
      "Length: 65, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "over_sampler = RandomOverSampler(random_state = 42)\n",
    "\n",
    "X_train_over,y_train_over=over_sampler.fit_resample(X_train,Y_train)\n",
    "print(X_train_over.isnull().sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_train_over.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m gb_clf \u001b[39m=\u001b[39m GradientBoostingClassifier(n_estimators\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_features\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, max_depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m gb_clf\u001b[39m.\u001b[39;49mfit(X_train_over, Y_train)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:429\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_state()\n\u001b[0;32m    425\u001b[0m \u001b[39m# Check input\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39m# Since check_array converts both X and y to the same dtype, but the\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[39m# trees use different types for X and y, checking them separately.\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    430\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mDTYPE, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    431\u001b[0m )\n\u001b[0;32m    433\u001b[0m sample_weight_is_none \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    435\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(n_estimators=20, learning_rate=1, max_features=2, max_depth=2, random_state=0)\n",
    "gb_clf.fit(X_train_over, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimal Parameter search for the Model ##\n",
    "\n",
    "# Parameters: \"precision\"  \"recall\" \"support\"\n",
    "\n",
    "# Define the parameter grid\n",
    "\n",
    "param_grid = {\n",
    " 'n_estimators': [250,300],\n",
    "\n",
    " 'learning_rate': [0.2, 0.4,0.6,0.8 ],\n",
    "\n",
    " 'max_depth': [2 , 3 , 4,5 ]\n",
    "\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingClassifier object\n",
    "\n",
    "#gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "\n",
    "grid_search = GridSearchCV(gb_clf, param_grid, cv=5, scoring=\"recall\", n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "print(\"Best score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reporting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
